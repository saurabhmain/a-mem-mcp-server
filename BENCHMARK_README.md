# Ollama Model Benchmark Tool

Modernes TUI (Text User Interface) fÃ¼r Geschwindigkeits-Tests von Ollama-Modellen.

## ğŸš€ Features

- âœ… **Moderne TUI** mit Textual Framework
- âœ… **Live Metriken**: Tokens/sec, Latency, First Token Time
- âœ… **Multi-Model Testing**: Teste verschiedene Modelle nacheinander
- âœ… **Results Export**: Speichere Ergebnisse als JSON
- âœ… **Real-time Progress**: Live Progress Bar wÃ¤hrend Benchmark
- âœ… **Interactive Logs**: Detaillierte Logs fÃ¼r jeden Test

## ğŸ“‹ Installation

```bash
# Installiere Dependencies
pip install textual requests

# Oder nutze requirements.txt
pip install -r requirements.txt
```

## ğŸ¯ Verwendung

```bash
# Starte das Benchmark Tool
python ollama_benchmark.py
```

### TUI Navigation

- **Model auswÃ¤hlen**: Dropdown-MenÃ¼ oben
- **Prompt anpassen**: Text-Input fÃ¼r Test-Prompt
- **Benchmark starten**: 
  - Button "ğŸš€ Run Benchmark" klicken
  - Oder Taste `r` drÃ¼cken
- **Results lÃ¶schen**: 
  - Button "ğŸ—‘ï¸ Clear Results" klicken
  - Oder Taste `c` drÃ¼cken
- **Results speichern**: 
  - Button "ğŸ’¾ Save Results" klicken
  - Oder Taste `s` drÃ¼cken
- **Beenden**: Taste `q` drÃ¼cken

## ğŸ“Š Metriken

Das Tool misst folgende Performance-Metriken:

- **Tokens/sec**: Generierungsgeschwindigkeit
- **Total Time**: Gesamte Antwortzeit
- **Tokens**: Anzahl generierter Tokens
- **First Token Time**: Zeit bis zum ersten Token (TTFT)
- **Avg Token Time**: Durchschnittliche Zeit pro Token

## ğŸ’¾ Export

Results werden als JSON gespeichert:

```json
[
  {
    "model": "qwen3:4b",
    "prompt": "Write a short story...",
    "total_time": 12.34,
    "tokens_generated": 87,
    "tokens_per_second": 7.05,
    "first_token_time": 0.234,
    "avg_token_time": 0.142,
    "timestamp": "2025-11-27T04:00:00"
  }
]
```

## ğŸ¨ Screenshots

Das TUI zeigt:
- Model-Selektor
- Prompt-Editor
- Live Progress Bar
- Results-Tabelle mit allen Metriken
- Detaillierte Logs

## ğŸ”§ Anpassungen

### Custom Prompts

Du kannst den Standard-Prompt im Code Ã¤ndern:

```python
current_prompt = reactive("Dein Custom Prompt hier...")
```

### Max Tokens

Standard: 100 Tokens. Ã„ndere in `run_benchmark()`:

```python
result = await loop.run_in_executor(
    None,
    benchmark.benchmark_model,
    self.current_model,
    self.current_prompt,
    200  # max_tokens anpassen
)
```

## ğŸ› Troubleshooting

**"No models found"**
- Stelle sicher, dass Ollama lÃ¤uft: `ollama serve`
- PrÃ¼fe, ob Modelle installiert sind: `ollama list`

**"Connection refused"**
- PrÃ¼fe Ollama URL (Standard: `http://localhost:11434`)
- Ã„ndere `OLLAMA_BASE_URL` im Code falls nÃ¶tig

**Benchmark hÃ¤ngt**
- PrÃ¼fe Ollama Logs
- Stelle sicher, dass genug RAM/VRAM verfÃ¼gbar ist

## ğŸ“ Beispiel-Output

```
Model          | Tokens/sec | Total Time | Tokens | First Token | Avg Token
qwen3:4b       | 7.05       | 12.34      | 87     | 0.234       | 142.00
llama3.2:3b    | 12.45      | 8.03       | 100    | 0.189       | 80.30
mistral:7b     | 5.23       | 19.12      | 100    | 0.456       | 191.20
```

## ğŸ¯ Best Practices

1. **Warm-up**: Erste Anfrage kann langsamer sein (Model Loading)
2. **Konsistenz**: Nutze denselben Prompt fÃ¼r faire Vergleiche
3. **Mehrere Runs**: FÃ¼hre mehrere Benchmarks aus fÃ¼r Durchschnittswerte
4. **System Load**: SchlieÃŸe andere GPU-intensive Apps wÃ¤hrend Tests

## ğŸ“š Technische Details

- **Framework**: Textual (moderne Python TUI Library)
- **API**: Ollama REST API (`/api/generate`)
- **Streaming**: Nutzt Streaming fÃ¼r prÃ¤zise Token-Messung
- **Async**: Asynchrone AusfÃ¼hrung fÃ¼r responsive UI

## ğŸ”— Links

- [Textual Documentation](https://textual.textualize.io/)
- [Ollama API Docs](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Ollama Models](https://ollama.com/library)



